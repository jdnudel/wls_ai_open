{"cells":[{"cell_type":"markdown","source":[" # part 3: dataset construction"],"metadata":{}},{"cell_type":"markdown","source":[" for each variable, split the data into training, validation, and test sets\n","\n"," * use two validation sets so we can train, validate, and pseudo-test before the final testing stage.\n"," * [this](https://cs230-stanford.github.io/train-dev-test-split.html) is a nice guide to splitting data for a machine learning project\n",""],"metadata":{}},{"source":["import pandas as pd\n","import numpy as np\n","import random\n","from sklearn.utils import shuffle\n","from sklearn.model_selection import train_test_split\n","import python_modules.constants as constants\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["np.random.seed(seed=2227)\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["PATH = 'study_data/'\n","\n","feature_set_names = [0,1,2,3]\n","label_set_names   = ['train', 'val_1', 'val_2', 'test']\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["df_main = pd.read_csv(f'{PATH}/study_data.csv', low_memory=False, index_col=0)\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["df_leak = df_main['LEAK']\n","df_clot = df_main['CLOT']\n","\n","rs_leak = [np.random.randint(500, size=1)[0] for i in range(0,3)]\n","rs_clot = [np.random.randint(500, size=1)[0] for i in range(0,3)]\n","\n","df_main = df_main.drop(columns=constants.OUTCOME)\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["rs_clot\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" split out the data. since the outcomes of interest are so rare, stratify by outcome to ensure even splits.\n","\n"," in order to do it totally evenly, each modeled outcome gets its own dataset\n","\n"," * this probably isn't necessary and probably doesn't help model. to get rid of stratification, delete the stratify kwarg"],"metadata":{}},{"source":["def data_splitter(features, labels, outcome, rstates):\n","        \n","    X_train, X_test, y_train, y_test   = train_test_split(features, labels, test_size=1/4, stratify=labels, random_state=rstates[0])\n","    X_train, X_val_1, y_train, y_val_1 = train_test_split(X_train, y_train, test_size=1/3, stratify=y_train, random_state=rstates[1])\n","    X_train, X_val_2, y_train, y_val_2 = train_test_split(X_train, y_train, test_size=1/2, stratify=y_train, random_state=rstates[2])\n","    \n","    \n","    X_train = X_train.merge(y_train.to_frame(), left_index=True, right_index=True)\n","    X_val_1 = X_val_1.merge(y_val_1.to_frame(), left_index=True, right_index=True)\n","    X_val_2 = X_val_2.merge(y_val_2.to_frame(), left_index=True, right_index=True)\n","    X_test = X_test.merge(y_test.to_frame(), left_index=True, right_index=True)\n","    \n","    consolidated_sets = [X_train, X_val_1, X_val_2, X_test]\n","\n","    # make sure the split sets are sized properly\n","    print([len(i) for i in consolidated_sets])\n","    \n","    # inspect outcome incidences in each split set\n","    print([i[outcome].value_counts()[True]/len(i) for i in consolidated_sets])\n","    \n","    return consolidated_sets\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["def data_splitter_2(features, labels, outcome, rstates):\n","        \n","    X_train, X_test, y_train, y_test   = train_test_split(features, labels, test_size=1/4, stratify=labels, random_state=rstates[0])\n","    X_train, X_val_1, y_train, y_val_1 = train_test_split(X_train, y_train, test_size=1/3, stratify=y_train, random_state=rstates[1])\n","    X_train, X_val_2, y_train, y_val_2 = train_test_split(X_train, y_train, test_size=1/2, stratify=y_train, random_state=rstates[2])\n","    \n","    \n","    X_train = X_train.merge(y_train.to_frame(), left_index=True, right_index=True)\n","    X_val_1 = X_val_1.merge(y_val_1.to_frame(), left_index=True, right_index=True)\n","    X_val_2 = X_val_2.merge(y_val_2.to_frame(), left_index=True, right_index=True)\n","    X_test = X_test.merge(y_test.to_frame(), left_index=True, right_index=True)\n","    \n","    consolidated_sets = [X_train, X_val_1, X_val_2, X_test]\n","\n","    # make sure the split sets are sized properly\n","    print([len(i) for i in consolidated_sets])\n","    \n","    # inspect outcome incidences in each split set\n","    print([i[outcome].value_counts()[True]/len(i) for i in consolidated_sets])\n","    \n","    return consolidated_sets\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["leak_sets = data_splitter(df_main, df_leak, 'LEAK', rs_leak)\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["clot_sets = data_splitter(df_main, df_clot, 'CLOT', rs_clot)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" build dataframes back up and save them"],"metadata":{}},{"source":["for i,j,k in zip(leak_sets, clot_sets, label_set_names):\n","    i['LEAK_SET'] = k\n","    j['CLOT_SET'] = k\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["concat_leak = pd.concat(leak_sets)\n","concat_clot = pd.concat(clot_sets)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" test to make sure the data is organized as anticipated. we are looking for where the val_2 set transitions into the test set.\n","\n"," * uncomment the following two cells to see the pertinent portions of these tables"],"metadata":{}},{"source":["#concat_leak[len(leak_sets[0])*3 - 2:len(leak_sets[0])*3 + 2]\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["#concat_clot[len(clot_sets[0])*3 - 2:len(clot_sets[0])*3 + 2]\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["df_main = pd.merge(concat_leak, concat_clot[['CLOT', 'CLOT_SET']], left_index=True, right_index=True)\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["len(df_main.columns)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" re-index"],"metadata":{}},{"source":["df_main = df_main.reset_index(drop=True)\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["len(df_main)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" save"],"metadata":{}},{"source":["df_main.to_csv(f'{PATH}/study_data_split.csv')\n","\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}