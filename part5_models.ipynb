{"cells":[{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["from IPython import get_ipython\n",""]},{"cell_type":"markdown","metadata":{},"source":[" # part 5: models\n","\n"," construct gastrointestinal leak and VTE risk prediction models\n","\n"," * 'analysis populations' refer to the training, validation, and test populations"]},{"cell_type":"markdown","metadata":{},"source":[" ## 1. preliminaries"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["get_ipython().run_line_magic('matplotlib', 'inline')\n","get_ipython().run_line_magic('reload_ext', 'autoreload')\n","get_ipython().run_line_magic('autoreload', '2')\n","\n",""]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["#from fastai.structured import *\n","#from fastai.column_data import *\n","import numpy as np\n","import pandas as pd\n","np.set_printoptions(threshold=50, edgeitems=20)\n","\n",""]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["from pandas.api.types import CategoricalDtype\n","from sklearn.preprocessing import StandardScaler\n","from imblearn.over_sampling import RandomOverSampler #ADASYN, SMOTE\n","import python_modules.constants as constants\n","from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, accuracy_score, auc\n","from sklearn.linear_model import LogisticRegression\n","from xgboost import XGBClassifier\n","import torch as torch\n","from torch.utils.data import DataLoader\n","import torch.nn as nn\n","import matplotlib.pyplot as plt\n","import os\n","\n",""]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["from python_modules.pytorch_tabular import TabularDataset, FeedForwardNN \n",""]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["# Set ipython's max row display\n","pd.set_option('display.max_row', 100)\n","\n","# Set iPython's max column display\n","pd.set_option('display.max_columns', 50)\n","\n",""]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["# seed random number generators\n","np.random.seed(6478)\n","torch.manual_seed(2368)\n","xgb_seed_leak = 32457\n","xgb_seed_clot = 21345\n","\n",""]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["# Device configuration - included as a formality. we did not use a GPU so running on GPU is not actually tested. \n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu'); device\n",""]},{"cell_type":"markdown","metadata":{},"source":[" ## 2. configure features and outcomes\n","\n"," specify features to be used in each model and outcome to be predicted. these specifications dictate data processing and model building."]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["cat_vars = constants.CATEGORICAL_PRE\n","con_vars = constants.CONTINUOUS_PRE\n","cat_ord  = constants.CATEGORICAL_ORDER\n",""]},{"cell_type":"markdown","metadata":{},"source":[" ## 3.  data processing\n","\n"," * note: 'analysis populations' refer to the training, validation, and test populations"]},{"cell_type":"markdown","metadata":{},"source":[" ### 3.1 format and resample data for handling by models"]},{"cell_type":"markdown","metadata":{},"source":[" get data into correct types"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["def change_type(df, cats, cons, cat_ord, out, out_set):\n","    \"\"\"\n","    gets features into correct data type\n","    also selects down to appropriate parameters\n","    ----------\n","\n","    \"\"\"\n","    for v in cats:\n","        df[v] = df[v].astype(CategoricalDtype(cat_ord[v], ordered=True))\n","    df[cats] = np.stack([c.cat.codes.values for n,c in df[cats].items()], 1).astype(np.int64)\n","    for v in cons:\n","        df[v] = df[v].astype('float32')\n","    return df[cats + cons + [out] + [out_set]]\n",""]},{"cell_type":"markdown","metadata":{},"source":[" reindex dataframe by analysis population"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["def re_index(df, out_set):\n","    df.set_index(out_set, append=True, inplace=True, verify_integrity=False)\n","    return df\n",""]},{"cell_type":"markdown","metadata":{},"source":[" standardize  continuous data, get a scaling function to map  validation and testing data\n","\n"," * group by analysis population\n"," * scale and normalize test population; generate a standardization function to map transforms to other analysis populations\n"," * assume population mean for missing continous data\n"," * apply standardization to validation and testing populations\n"," * set `nan` to training population average, which is zero after standardization\n"," * confirm no `nan` remain - we set null continuous feature instances to the training population mean in the last step of the standardization function."]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["def standardize(df, scaler_fn, cons, out_set):\n","    #stratify by analysis pop\n","    #if you don't copy, overwrite errors ensue\n","    #use 50% of data for training, 25% for validation, and 25% for testing\n","    df_tr = df.xs('train', level=out_set).copy()\n","    df_tr2 = df.xs('val_1', level=out_set).copy()\n","    df_tr = pd.concat([df_tr, df_tr2])\n","    \n","    df_v = df.xs('val_2', level=out_set).copy()\n","    df_test = df.xs('test', level=out_set).copy()\n","    \n","    df_tr[cons] = scaler_fn.fit_transform(df_tr[cons].values)\n","    \n","    df_v[cons] = scaler_fn.transform(df_v[cons].values)\n","    df_test[cons] = scaler_fn.transform(df_test[cons].values)\n","    \n","    #zero out the unknown continuous variables; this sets them to training population average\n","    for v in cons:\n","        df_tr[v] = df_tr[v].fillna(0).astype('float32')\n","        df_v[v] = df_v[v].fillna(0).astype('float32')\n","        df_test[v] = df_test[v].fillna(0).astype('float32')\n","        \n","    #confirm no nan remain\n","    print('the number of nan remaining is: ' + str(df_tr.isnull().values.sum()))\n","    \n","    return df_tr, df_v, df_test\n",""]},{"cell_type":"markdown","metadata":{},"source":[" balance\n"," * we can use pandas for simple oversampling. imbalanced-learn library implements advanced oversampling algorithms.\n"," * if using a synthetic oversampler, choose one that can synthesize categorical variables since most of the features in our model are categorical\n"," * [imblearn oversampling algorithms](https://imbalanced-learn.readthedocs.io/en/stable/over_sampling.html#smote-adasyn)\n"," * [good blog post on how to do this](https://beckernick.github.io/oversampling-modeling/)\n","\n"," <b>Oversample only the training set. There is no reason to oversample any other set. Do not validate or test against oversampled data. Doing so will invalidate the model. </b>\n","\n"," imbalance-learn appends resampled instances to the end of the data. (to prove this, run `df_tr_resam[-50:-1]['LEAK']` before and after shuffling). therefore, after resampling, reshuffle the oversampled data"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["def balance(df_tr, oversamp_fn, cats, cons, out_v):\n","    #split features from outcomes\n","    df_tr_y = df_tr[out_v].copy()\n","    df_tr_X = df_tr.drop([out_v], axis=1)\n","    df_tr_resam_X, df_tr_resam_y = oversamp_fn.fit_resample(df_tr_X, df_tr_y)\n","    \n","    #resampled data to dataframe\n","    df_tr_resam = pd.DataFrame(df_tr_resam_X, columns = df_tr_X.columns)\n","    \n","    #add outcome variable back to dataframe\n","    df_tr_resam[out_v] = df_tr_resam_y\n","    \n","    #shuffle\n","    df_tr_resam = df_tr_resam.sample(frac=1).reset_index(drop=True)\n","    \n","    #get resampled data back into the right type (it gets transformed in the resampling function)\n","    for v in cons:\n","        df_tr_resam[v] = df_tr_resam[v].astype('float32')\n","    for v in cats:\n","        df_tr_resam[v] = df_tr_resam[v].astype('int64')\n","\n","    return df_tr_resam\n","\n",""]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["def get_incidence_of_outcome(s):\n","    \"\"\"\n","    Calculates incidence of outcome variable in a series\n","    Parameters\n","    ----------\n","    s: pandas series\n","      the series object containing boolean values for\n","      outcome of interest. \n","    \"\"\"\n","\n","    cases = s.value_counts()[1]\n","    total = s.count()\n","    return cases, cases / total\n",""]},{"cell_type":"markdown","metadata":{},"source":[" ### 3.2 import data"]},{"cell_type":"markdown","metadata":{},"source":[" process the data using the steps described above\n","\n"," * the reason we print incidences of outcomes of interest in training, validation, and testing sets in this step is because after this step, cases are oversampled in the training set."]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["df_main = pd.read_csv('study_data/study_data_split.csv', low_memory=False, index_col=0)\n","\n",""]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["def data_for_models(categorical_vars, categorical_order, continuous_vars, outcome_var, outcome_set):\n","    \n","    cat_vars = categorical_vars\n","    cat_vars_ord = categorical_order\n","    con_vars = continuous_vars\n","    outcome_var = outcome_var\n","    outcome_set = outcome_set\n","    \n","    df_main = pd.read_csv('study_data/study_data_split.csv', low_memory=False, index_col=0)\n","    scaler = StandardScaler()\n","    \n","    oversamp_ratio = 0.5\n","    oversampler = RandomOverSampler(ratio = oversamp_ratio)\n","    \n","    df_main = change_type(df_main, cat_vars, con_vars, cat_vars_ord, outcome_var, outcome_set)\n","        \n","    df_main = re_index(df_main, outcome_set)\n","    df_train, df_validate, df_test = standardize(df_main, scaler, con_vars, outcome_set)\n","    \n","    print('incidence in training set -', get_incidence_of_outcome(df_train[outcome_var]))\n","    print('incidence in validation set -', get_incidence_of_outcome(df_validate[outcome_var]))\n","    print('incidence in testing set -', get_incidence_of_outcome(df_test[outcome_var]))\n","    \n","    df_train_rebalanced = balance(df_train, oversampler, cat_vars, con_vars, outcome_var)\n","    \n","    return df_train_rebalanced, df_validate, df_test, df_train\n",""]},{"cell_type":"markdown","metadata":{},"source":[" reformat the data so it can be used in all analyses below:"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["def import_data(outcome_var, outcome_set):\n","    \n","    data = {}\n","    \n","    training, validation, testing, training_original = data_for_models(cat_vars, cat_ord, con_vars, outcome_var, outcome_set)\n","    \n","    # 2019-09-23 for revisions:\n","    # save the data generated to train models for each outcome\n","    # this will be used to generate new logistic regression models\n","    # this is to respond to reviewer comments asking for more info on LR coefficients\n","    print('export scaled & resampled data for analysis with statsmodels')\n","    training.to_csv('study_data/' + outcome_var + '_training.csv')\n","    validation.to_csv('study_data/' + outcome_var + '_validation.csv')\n","    testing.to_csv('study_data/' + outcome_var + '_testing.csv')\n","    training_original.to_csv('study_data/' + outcome_var + '_training_original.csv')\n","    \n","    # original submission code:\n","    \n","    train_labels = training[outcome_var]\n","    train_features = training.drop(outcome_var, axis=1)\n","\n","    valid_labels = validation[outcome_var]\n","    valid_features = validation.drop(outcome_var, axis=1)\n","\n","    testing_labels = testing[outcome_var]\n","    testing_features = testing.drop(outcome_var, axis=1)\n","\n","    training_original_labels = training_original[outcome_var]\n","    training_original_features = training_original.drop(outcome_var, axis=1)\n","    \n","    data['training'] = training\n","    data['validation'] = validation\n","    data['testing'] = testing\n","    data['training_original'] = training_original\n","    \n","    data['train_labels'] = train_labels\n","    data['train_features'] = train_features\n","    data['valid_labels'] = valid_labels\n","    data['valid_features'] = valid_features\n","    data['testing_labels'] = testing_labels\n","    data['testing_features'] = testing_features\n","    \n","    # the train_original_ label allows us to use non-oversampled training data to check model performance\n","    # on the original training cohort (just to make sure results make sense and that we're not overfitting etc)\n","    data['train_original_labels'] = training_original_labels\n","    data['train_original_features'] = training_original_features\n","    return data\n",""]},{"cell_type":"markdown","metadata":{},"source":[" ## 4. configure models"]},{"cell_type":"markdown","metadata":{},"source":[" ### 4.1 artificial neural network"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["def prep_train_data(data, features_cat, features_con, outcome):\n","    d_trv = data[features_cat + features_con + [outcome]]\n","    d_trv = TabularDataset(data=d_trv, cat_cols=features_cat, output_col=outcome)\n","    d_trv = DataLoader(d_trv, 640, shuffle=True, num_workers=1)\n","    return d_trv\n","\n","def prep_test_val_data(data, features_cat, features_con, outcome):\n","    batch_size = len(data)\n","    d_trv = data[features_cat + features_con + [outcome]]\n","    d_trv = TabularDataset(data=d_trv, cat_cols=features_cat, output_col=outcome)\n","    d_trv = DataLoader(d_trv, batch_size, shuffle=False, num_workers=1)\n","    return d_trv\n","\n","def get_net(data, features_cat, features_con):\n","    cat_dims = [int(data[col].nunique()) for col in features_cat]\n","    emb_dims = [(x+1, 5) for x in cat_dims]\n","    lls = [550]\n","    lld = [0.4 for i in lls]\n","    net = FeedForwardNN(emb_dims, no_of_cont=len(features_con), \n","                    lin_layer_sizes = lls,\n","                    output_size=1, emb_dropout=0.2,\n","                    lin_layer_dropouts = lld).to(device)\n","    return net\n","\n","def ann_train(net, crit, opt, training_data, losses_train):\n","    net.train()\n","    epoch_loss = []\n","    \n","    for y, cont_x, cat_x in training_data:\n","        cat_x = cat_x.to(device)\n","        cont_x = cont_x.to(device)\n","        y = y.to(device)\n","    \n","        # Forward Pass\n","        preds = net(cont_x, cat_x)\n","        loss = crit(preds, y)\n","    \n","        # Backward Pass and Optimization\n","        opt.zero_grad()\n","        loss.backward()\n","        opt.step()\n","        # record losses\n","        epoch_loss.append(loss.data.numpy())\n","    \n","    losses_train.append(np.mean(epoch_loss))\n","\n","def evaluate(net, crit, val_data, losses_val, aucs_val):\n","    net.eval()\n","    epoch_loss = []\n","    \n","    for y, cont_x, cat_x in val_data:\n","        cat_x = cat_x.to(device)\n","        cont_x = cont_x.to(device)\n","        y = y.to(device)\n","        preds = net(cont_x, cat_x)\n","        loss = crit(preds, y)                    \n","        pred_y = preds.data.numpy()\n","        target_y = y.data.numpy()  \n","        epoch_loss.append(loss.data.numpy())\n","        \n","    fpr, tpr, _ = roc_curve(target_y, pred_y)\n","    aucs_val.append(auc(fpr, tpr))\n","    losses_val.append(np.mean(epoch_loss))\n","    \n","def ann_test(net, crit, test_data):\n","    net.eval()\n","    epoch_loss = []\n","    \n","    for y, cont_x, cat_x in test_data:\n","        cat_x = cat_x.to(device)\n","        cont_x = cont_x.to(device)\n","        y = y.to(device)\n","        preds = net(cont_x, cat_x)\n","        loss = crit(preds, y)                    \n","        pred_y = preds.data.numpy()\n","        target_y = y.data.numpy()  \n","        epoch_loss.append(loss.data.numpy())\n","        mean_loss = np.mean(epoch_loss)\n","            \n","    results = {'preds': pred_y,\n","               'loss': mean_loss}\n","    \n","    return results\n","        \n","def train_and_val(net, dl_train, dl_valid, path_to_model, patience = 4, max_its = 20, min_its = 0):\n","    opt_slow = torch.optim.Adam(net.parameters(), lr=0.0001)\n","    criterion = nn.BCELoss()\n","    losses_train = []\n","    losses_val = []\n","    aucs_val = []\n","    best_model = []\n","    patience = patience\n","    eval_loss = 0\n","    max_its = max_its\n","    min_its = min_its\n","    it = 0\n","    save_best = False\n","    while it < max_its:\n","        ann_train(net, criterion, opt_slow, dl_train, losses_train)\n","        evaluate(net, criterion, dl_valid, losses_val, aucs_val)\n","        \n","        #save best model:\n","        if it == np.argmax(aucs_val):\n","            update = ', validation auc: ' + str(aucs_val[-1]) + '***'\n","            torch.save(net.state_dict(), path_to_model)\n","        \n","        else:\n","            update = ''\n","        \n","        #evaluate ending criteria:\n","        if it > min_its:\n","        \n","            if len(aucs_val) - np.argmax(aucs_val) > patience:\n","                #print('ending after epoch ' + str(it))\n","                print('END epoch ' + str(it) + update)\n","                print('the best model saved after epoch ' + str(np.argmax(aucs_val)))\n","                break\n","                \n","        print('epoch ' + str(it) + update)\n","        it += 1\n","    return losses_train, losses_val, aucs_val\n","        \n",""]},{"cell_type":"markdown","metadata":{},"source":[" ### 4.2 gradient boosting machine"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["def xgboost_train(train_features, train_labels, xgb_seed):\n","    \n","    tf = train_features.copy()\n","    \n","    for v in cat_vars: \n","        tf[v] = tf[v].astype('str')\n","        \n","    tf = pd.get_dummies(tf, columns = cat_vars, drop_first=True )\n","    \n","    model = XGBClassifier(seed=xgb_seed)\n","    model.fit(tf, train_labels)\n","    var_names = list(tf)\n","    var_imps = model.feature_importances_#rf_feat_importance(model, tf)\n","    df_vi = pd.DataFrame({'var':var_names, 'imp':var_imps})\n","    \n","    \n","    \n","    return model, df_vi\n","\n",""]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["def xgboost_test(model, test_features):\n","\n","    vf = test_features.copy()\n","    \n","    for v in cat_vars: \n","        vf[v] = vf[v].astype('str')\n","    \n","    vf = pd.get_dummies(vf, columns = cat_vars, drop_first=True)\n","\n","    guess = model.predict_proba(vf) #compare it to test_target_cat\n","\n","    return guess[:,1]\n",""]},{"cell_type":"markdown","metadata":{},"source":[" ### 4.3 logistic regression"]},{"cell_type":"markdown","metadata":{},"source":[" standard encoding"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["def lr_fit(train_features, train_labels):\n","    \n","    coeffs = {}\n","    \n","    clf = LogisticRegression(solver='saga', max_iter=9000)\n","    clf.fit(train_features, train_labels)\n","    coeffs['names'] = list(train_features)\n","    coeffs['vals'] = clf.coef_\n","    \n","    return clf, coeffs\n","\n","def lr_test(model, test_features):\n","    guess_lr = model.predict_proba(test_features) #compare it to test_target_cat\n","    return guess_lr[:,1]\n",""]},{"cell_type":"markdown","metadata":{},"source":[" one-hot encoding"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["def lr_fit_onehot(train_features, train_labels):\n","    \n","    tf = train_features.copy()\n","    \n","    for v in cat_vars: \n","        tf[v] = tf[v].astype('str')\n","        \n","    tf = pd.get_dummies(tf, columns = cat_vars, drop_first=True)\n","    \n","    coeffs = {}\n","    \n","    clf = LogisticRegression(solver='saga', max_iter=9000)\n","    clf.fit(tf, train_labels)\n","    coeffs['names'] = list(train_features)\n","    coeffs['vals'] = clf.coef_\n","    \n","    return clf, coeffs\n","\n","def lr_test_onehot(model, test_features):\n","    tf = test_features.copy()\n","    \n","    for v in cat_vars: \n","        tf[v] = tf[v].astype('str')\n","        \n","    tf = pd.get_dummies(tf, columns = cat_vars, drop_first=True)\n","    \n","    guess_lr = model.predict_proba(tf) #compare it to test_target_cat\n","    return guess_lr[:,1]\n",""]},{"cell_type":"markdown","metadata":{},"source":[" ### 4.4 utilities for running models and processing results"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["def run_models(outcome_var, outcome_set, output_path_modifier, xgb_seed):\n","    \n","    # specify paths for saving neural net and model results\n","    # make dirs to hold outputs\n","    # if folders already exist, this will throw errors\n","    # use separate subdirectories for leak and clot results otherwise confusing + annoying errors\n","    \n","    # path to best ANN in training and allows us to save and recover it once training loop ends\n","    PATH_MODEL = f'results/best_ann_{outcome_var}/' \n","    os.mkdir(PATH_MODEL)\n","    BEST_MODEL = f'{PATH_MODEL}ANN_{outcome_var}.pt'\n","    \n","    # path to  results\n","    PATH_RESULTS = f'results/study_models_{outcome_var}/'\n","    os.mkdir(PATH_RESULTS)\n","    \n","    #import data\n","    print('IMPORT DATA')\n","    data = import_data(outcome_var, outcome_set)\n","    \n","    #ann - train\n","    print('\\n')\n","    print('TRAIN NEURAL NET')\n","    ann_training = prep_train_data(data['training'], cat_vars, con_vars, outcome_var)\n","    ann_training_performance = prep_test_val_data(data['training_original'], cat_vars, con_vars, outcome_var)\n","    ann_validate = prep_test_val_data(data['validation'], cat_vars, con_vars, outcome_var)\n","    ann_testing = prep_test_val_data(data['testing'], cat_vars, con_vars, outcome_var)\n","\n","    ann = get_net(data['training'], cat_vars, con_vars)\n","    ann_losses_training, ann_losses_validate, ann_aucs_validate = train_and_val(ann, ann_training, ann_validate, BEST_MODEL, max_its = 900, patience = 100, min_its = 30)\n","\n","    # recover best ann\n","    ann_best = get_net(data['training'], cat_vars, con_vars)\n","    ann_best.load_state_dict(torch.load(BEST_MODEL))\n","    \n","    # plot ann training and validation losses\n","    print('\\n')\n","    print('NEURAL NET - TRAINING LOSS')\n","    plt.plot(ann_losses_training)\n","    plt.title('training loss', fontdict=None, loc='center', pad=None)\n","    plt.show()\n","    \n","    print('\\n')\n","    print('NEURAL NET - VALIDATION LOSS')\n","    plt.plot(ann_losses_validate)\n","    plt.title('validation loss', fontdict=None, loc='center', pad=None)\n","    plt.show()\n","    \n","    print('\\n')\n","    print('NEURAL NET - VALIDATION AUCS')\n","    plt.plot(ann_aucs_validate)\n","    plt.title('validation AUCs', fontdict=None, loc='center', pad=None)\n","    plt.show()\n","    \n","    # evaluate ann against validation and test data\n","    ann_train_results = ann_test(ann_best, nn.BCELoss(), ann_training_performance)\n","    ann_valid_results = ann_test(ann_best, nn.BCELoss(), ann_validate)\n","    ann_test_results = ann_test(ann_best, nn.BCELoss(), ann_testing)\n","    print('\\n')\n","    print('NEURAL NET - done')\n","        \n","    # xgb\n","    xgb_model, xgb_var_imp = xgboost_train(data['train_features'], data['train_labels'], xgb_seed)\n","    xgb_train_results = xgboost_test(xgb_model, data['train_original_features'])\n","    xgb_valid_results = xgboost_test(xgb_model, data['valid_features'])\n","    xgb_testing_results = xgboost_test(xgb_model, data['testing_features'])\n","    print('\\n')\n","    print('GRADIENT BOOSTING MACHINE - done') \n","    \n","    # lr\n","    lr_model, lr_var_coeffs = lr_fit(data['train_features'], data['train_labels'])\n","    lr_train_results = lr_test(lr_model, data['train_original_features'])\n","    lr_valid_results = lr_test(lr_model, data['valid_features'])\n","    lr_testing_results = lr_test(lr_model, data['testing_features'])\n","    print('\\n')\n","    print('LOGISTIC REGRESSION - done')\n","    \n","    # lr - onehot version\n","    lr_onehot_model, lr_onehot_var_coeffs = lr_fit_onehot(data['train_features'], data['train_labels'])\n","    lr_onehot_train_results = lr_test_onehot(lr_onehot_model, data['train_original_features'])\n","    lr_onehot_valid_results = lr_test_onehot(lr_onehot_model, data['valid_features'])\n","    lr_onehot_testing_results = lr_test_onehot(lr_onehot_model, data['testing_features'])\n","    print('\\n')\n","    print('ONE HOT LOGISTIC REGRESSION - done')  \n","    \n","    # dfs to hold results\n","    df_results_train = pd.DataFrame()\n","    df_results_valid = pd.DataFrame()\n","    df_results_test = pd.DataFrame()\n","    \n","    # labels\n","    df_results_train['targs'] = data['train_original_labels']\n","    df_results_valid['targs'] = data['valid_labels']\n","    df_results_test['targs'] = data['testing_labels']\n","    \n","    # store results in dataframes\n","    df_results_train['ann'] = ann_train_results['preds']\n","    df_results_valid['ann'] = ann_valid_results['preds']\n","    df_results_test['ann'] = ann_test_results['preds']\n","    df_results_train['xgb'] = xgb_train_results\n","    df_results_valid['xgb'] = xgb_valid_results\n","    df_results_test['xgb'] = xgb_testing_results\n","    df_results_train['lr'] = lr_train_results\n","    df_results_valid['lr'] = lr_valid_results\n","    df_results_test['lr'] = lr_testing_results\n","    df_results_train['lr_onehot'] = lr_onehot_train_results\n","    df_results_valid['lr_onehot'] = lr_onehot_valid_results\n","    df_results_test['lr_onehot'] = lr_onehot_testing_results\n","    \n","    # save results - so that they can be evaluated in R\n","    df_results_train.to_csv(f'{PATH_RESULTS}{outcome_var}_train_{output_path_modifier}.csv')\n","    df_results_valid.to_csv(f'{PATH_RESULTS}{outcome_var}_valid_{output_path_modifier}.csv')\n","    df_results_test.to_csv(f'{PATH_RESULTS}{outcome_var}_test_{output_path_modifier}.csv')\n","    \n","    print('\\n')\n","    print('SAVE RESULTS - done') \n","    return df_results_valid, df_results_test, ann_valid_results['loss'], ann_test_results['loss'], xgb_var_imp, lr_var_coeffs\n","\n",""]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["def generate_results_roc(y_test, y_score):\n","    fpr, tpr, _ = roc_curve(y_test, y_score)\n","    roc_auc = auc(fpr, tpr)\n","    plt.figure()\n","    plt.plot(fpr, tpr, label='ROC curve (area = %0.2f' + str(roc_auc))\n","    plt.plot([0, 1], [0, 1], 'k--')\n","    plt.xlim([0.0, .2])\n","    plt.ylim([0.0, .6])\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.title('Receiver operating characteristic curve')\n","    plt.show()\n","    print('AUC: ' , roc_auc)\n","\n","def generate_results_pr(y_test, y_score):\n","    precision, recall, _ = precision_recall_curve(y_test, y_score)\n","    plt.figure()\n","    # In matplotlib < 1.5, plt.fill_between does not have a 'step' argument\n","    step_kwargs = ({'step': 'post'}\n","                   if 'step' in signature(plt.fill_between).parameters\n","                   else {})\n","    plt.step(recall, precision, color='b', alpha=0.2,\n","             where='post')\n","    plt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n","    \n","    plt.xlabel('True Positive Rate')\n","    plt.ylabel('Positive Predictive Value')\n","    plt.ylim([0.0, 1.05])\n","    plt.xlim([0.0, 0.2])\n","    plt.title('Precision-recall curve')\n","    plt.show()\n","\n","def compare_rocs(results_frame, list_y_preds, y_targets):\n","    plt.figure(figsize=(20,10))\n","    for r in list_y_preds:\n","        fpr, tpr, _ = roc_curve(results_frame[y_targets], results_frame[r])\n","        roc_auc = auc(fpr, tpr)\n","        print(r + ' AUC: ' , roc_auc)\n","        plt.plot(fpr, tpr)\n","    plt.plot([0, 1], [0, 1], 'k--')\n","    plt.xlim([-0.05, 1.05])\n","    plt.ylim([-0.05, 1.05])\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.title('Receiver operating characteristic curve')\n","    plt.show()\n",""]},{"cell_type":"markdown","metadata":{},"source":[" ## 5. train and test models"]},{"cell_type":"markdown","metadata":{},"source":[" ### 5.1 leak"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["leak_outcome = 'LEAK'\n","leak_set = 'LEAK_SET'\n","\n",""]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["df_leak_res_val, df_leak_res_test, ann_leak_valid_loss, ann_leak_test_loss, xgb_var_imp_leak, lr_var_imp_leak = run_models(leak_outcome, leak_set, \n","                                                                                        'FINAL', \n","                                                                                        xgb_seed_leak)\n",""]},{"cell_type":"markdown","metadata":{},"source":[" compare validation and testing results for neural net"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["print('compare artificial neural net validation and testing loss to ensure they are reasonably close')\n","print('* validation loss = ' + str(ann_leak_valid_loss))\n","print('* testing loss = ' + str(ann_leak_test_loss))\n",""]},{"cell_type":"markdown","metadata":{},"source":[" preliminary comparison of results - final results processed in R for easier statistical comparison of AUCs."]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["compare_rocs(df_leak_res_val, ['ann', 'xgb', 'lr', 'lr_onehot'], 'targs')\n","\n",""]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["compare_rocs(df_leak_res_test, ['ann', 'xgb', 'lr', 'lr_onehot'], 'targs')\n",""]},{"cell_type":"markdown","metadata":{},"source":[" variable importance"]},{"cell_type":"markdown","metadata":{},"source":[" ### 5.2 clot"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["clot_outcome = 'CLOT'\n","clot_set = 'CLOT_SET'\n","\n",""]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["df_clot_res_val, df_clot_res_test, ann_clot_valid_loss, ann_clot_test_loss, xgb_var_imp_clot, lr_var_imp_clot = run_models(clot_outcome, \n","                                                                                        clot_set, 'FINAL', \n","                                                                                        xgb_seed_clot)\n",""]},{"cell_type":"markdown","metadata":{},"source":[" same as above, some preliminary analysis. first, compare neural net validation and testing results. then, compare various models. again, we process final results in R which has very nice tools for statistical comparison of AUCs."]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["print('compare artificial neural net validation and testing loss to ensure they are reasonably close')\n","print('* validation loss = ' + str(ann_clot_valid_loss))\n","print('* testing loss = ' + str(ann_clot_test_loss))\n","\n",""]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["compare_rocs(df_clot_res_val, ['ann', 'xgb', 'lr', 'lr_onehot'], 'targs')\n","\n",""]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["compare_rocs(df_clot_res_test, ['ann', 'xgb', 'lr',  'lr_onehot'], 'targs')\n",""]},{"cell_type":"markdown","metadata":{},"source":[" ### variable importance"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["#this is just a dict that helps format variable names for figures\n","var_imp_name_mapper = {'VENOUS_STASIS Yes': 'Venous stasis: Yes',\n","                       'FUNSTATPRESURG Partially Dependent': 'Functional status: Partially Dependent',\n","                       'HIP Yes': 'HTN: Yes',\n","                       'COPD Yes': 'COPD: Yes',\n","                       'SMOKER Yes': 'Smoker: Yes',\n","                       'race_PUF White': 'Race: White',\n","                       'DIABETES Insulin': 'Diabetes: Insulin-dependent',\n","                       'BMI_DELTA': 'Change in BMI',\n","                       'race_PUF Black or African American': 'Race: Black or African American',\n","                       'race_PUF American Indian or Alaska Native': 'Race: American Indian or Alaska Native',\n","                       'BMI_CONSOL': 'Preop BMI',\n","                       'ASSISTANT_TRAINING_LEVEL Minimally Invasive Surgery Fellow': '1st asst. training: MIS Fellow',\n","                       'CHRONIC_STEROIDS Yes': 'Chronic steroids: Yes',\n","                       'MOBILITY_DEVICE Yes': 'Limited ambulation: Yes',\n","                       'hispanic Yes': 'Hispanic ethnicity: Yes',\n","                       'PCARD Yes': 'Previous cardiac surgery: Yes',\n","                       'ASACLASS 2-Mild Disturb': 'ASA class: 2',\n","                       'DIABETES Non-Insulin': 'Diabetes: No insulin',\n","                       'ASSISTANT_TRAINING_LEVEL Resident (PGY 1-5+)': '1st asst. training: Resident',\n","                       'IVCF No': 'IVCF: No',\n","                       'IVCF IVC filter placed in anticipation of the metabolic or bariatric procedure': 'IVCF: Placed for surgery',\n","                       'hispanic No': 'Hispanic ethnicity: No',\n","                       'THERAPEUTIC_ANTICOAGULATION Yes': 'Anticoagulation: Yes',\n","                       'ASSISTANT_TRAINING_LEVEL Attending - Other': '1st asst. training: attending',\n","                       'SLEEP_APNEA Yes': 'Sleep apnea: Yes',\n","                       'HTN_MEDS 2': 'No. anti-HTN: 2',\n","                       'HGTCM': 'Height (cm)',\n","                       'AGE_CONSOL': 'Age',\n","                       'FUNSTATPRESURG Totally Dependent':'Functional status: Dependent',\n","                       'ALBUMIN': 'Albumin',\n","                       'SEX Male': 'Sex: Male',\n","                       'HCT': 'Hematocrit',\n","                       'race_PUF American Indian or Alaska Native': 'Race: American Indian or Alaska Native',\n","                       'race_PUF Native Hawaiian or Other Pacific Islander': 'Race: Native Hawaiian or Other Pacific Islander',\n","                       'HISTORY_PE Yes': 'History of PE: Yes',\n","                       'HISTORY_DVT Yes' :'History of DVT: Yes',\n","                       'OXYGEN_DEPENDENT Yes' : 'Oxygen dependent: Yes',\n","                       'MI_ALL_HISTORY Yes':'History of MI: Yes',\n","                       'GERD Yes': 'GERD: Yes',\n","                       'ASSISTANT_TRAINING_LEVEL Attending - Weight Loss Surgeon': '1st asst. training: weight loss surgeon',\n","                       'CPT 43644': 'Procedure: Bypass',\n","                       'WGTKG': 'Weight (kg)',\n","                       'OPYEAR 2016': 'Year: 2016',\n","                       'IVCF IVC filter was pre-existing': 'IVCF: Pre-existing',\n","                       'PTC Yes' : 'Previous PCI or angioplasty',\n","                       'RENAL_INSUFFICIENCY Yes': 'Renal insufficiency: No',\n","                       'OPYEAR 2017':'Year: 2017',\n","                       'ASACLASS 1-No Disturb': 'ASA class: 1',\n","                       'ASACLASS 4-Life Threat': 'ASA class: 4',\n","                       'ASACLASS 3-Severe Disturb': 'ASA class: 3',\n","                       'HTN_MEDS 3+': 'No. anti-HTN: 3+',\n","                       'ASSISTANT_TRAINING_LEVEL Physician Assistant/Nurse Practitioner/Registered Nurse First Assist': '1st asst. training: PA/NP/RN ',\n","                       'HYPERLIPIDEMIA Yes': 'HLD: Yes',\n","                       'HTN_MEDS 1': 'No. anti-HTN: 0',\n","                       'DIALYSIS Yes': 'Dialysis: Yes'}\n","\n",""]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["def get_variable_importance(vardata, outcome_path):\n","    vardata_filtered = vardata[vardata['imp'] != 0].copy()\n","    relative_imp = [i/np.max(vardata_filtered['imp']) for i in vardata_filtered['imp']]\n","    vardata_filtered['relative_imp'] = relative_imp\n","    mapnames = []\n","    \n","    # requires some str manipulation to match strings and to get things into the right format\n","    for v in vardata_filtered['var']:\n","        if v[0:-2] in cat_vars:\n","            rank = int(v[-1:])\n","            n = var_imp_name_mapper[v[0:-2] + ' ' + str(cat_ord[v[0:-2]][rank])]\n","            mapnames += [n]\n","        else:\n","            mapnames += [var_imp_name_mapper[v]]\n","    vardata_filtered['names'] = mapnames\n","    vardata_filtered.to_csv(f'results/study_models_{outcome_path}/{outcome_path}_xgb_var_imp.csv')\n","    return vardata_filtered\n","\n",""]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["var_imp_leak = get_variable_importance(xgb_var_imp_leak, 'LEAK')\n","var_imp_clot = get_variable_importance(xgb_var_imp_clot, 'CLOT')\n","\n",""]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["\n","\n","\n",""]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}