# wls_ai_open

This repository contains code to build and test the models reported in our manuscript [_Development and Validation of Machine Learning Models to Predict Gastrointestinal Leak and Venous Thromboembolism After Weight Loss Surgery: An Analysis of the MBSAQIP Database_](https://link.springer.com/article/10.1007/s00464-020-07378-x).

If you use this code in your research, please cite our paper: Nudel J, Bishara AM, de Geus SWL, Patil P, Srinivasan J, Hess DT, Woodson J. Development and validation of machine learning models to predict gastrointestinal leak and venous thromboembolism after weight loss surgery: an analysis of the MBSAQIP database. Surgical Endoscopy. 2020; 10.1007/s00464-020-07378-x

Due to the Data Use Agreement associated with use of the Metabolic and Bariatric Surgery Accreditation and Quality Improvement Program Participant Use Files, we cannot post the underlying data here. However, anyone with access to the Metabolic and Bariatric Surgery Accreditation and Quality Improvement Program Participant Use Files can build the models reported in our paper using the code in this repository. 

## 1. configuration and installation

these instructions assume: 
* an iOS operating system
* models are trained on a CPU

### download major dependencies

* download Anaconda Distribution with python 3 by following the instructions [here](https://www.anaconda.com/distribution/)

* download RStudio Desktop [here](https://www.rstudio.com/products/rstudio/download/)

### environment setup

the following terminal commands will generate a new python 3.6 environment called `wls_ai` and activate it (the name of the environment is arbitrary, feel free to change it):

```
conda create -n wls_ai python=3.6
conda activate wls_ai
```

### install python dependencies

once your environment is created and activated, run the following terminal commands to install the dependencies used in this project:

```
pip install scipy numpy pandas matplotlib
pip install jupyter
conda install pytorch torchvision -c pytorch
pip install scikit-learn
pip install imbalanced-learn
pip install tablebone
pip install xgboost
conda install -c conda-forge statsmodels
```

### install required R packages

using the `packages` tab in RStudio (or whatever method works for you), install the following R packages:

* `pROC`
* `ggplot2`
* `dplyr`

## 2. organize the original data files

in a folder called **`/mbsaqip_originals`**: build .txt files from MBSAQIP executables and organize them by year for each year. Data for each year should be organized as shown in the 2015 folder:

    .
    ├── mbsaqip_originals
        ├── 2015
            ├── bmi.txt
            ├── intv.txt
            ├── main.txt
            ├── read.txt
            └── reop.txt
        ├── 2016
        └── 2017
    ├── part1_parse-original.ipynb
    ├── part2_pop-feat.ipynb
    ├── part3_build-datasets.ipynb
    ├── part4_descriptive-statistics-bariclot
    ├── part5_models.ipynb
    ├── part6_data_processing.R
    ├── python_modules
    ├── LICENSE
    └── README.md

## 3. start jupyter notebook

with the environment activated and the data structured as demonstrated above, return to the terminal and run `jupyter notebook` to start the Jupyter Notebook. navigate to the first notebook and start running the code. there are many resources and tutorials available for those new to using Jupyter Notebooks.

## 4. parse and join the data from all years

open **`part1_parse-original.ipynb`** and run all cells. This will create an **`/mbsaqip_originals/all_years`** directory under in which a file containing joined original data for all years, **`all_years.csv`**, will be saved.

## 5. population selection and feature engineering

open **`part2_pop-feat.ipynb`** and run all cells. This notebook: 

* handles missing data
* consolidates variables and does minor feature engineering 
* applies inclusion / exclusion criteria
* In the end, we create an **`/study_data`** directory in which a file containing joined data for all years,  **`study_data.csv`**, will be saved.

## 6. split the data  

open **`part3_build-datasets.ipynb`** and run all cells:

* for each variable, split the data into training, validation, and test sets
* save the resultant file in the **`/study_data`** directory as **`study_data_split.csv`**

## 7. generate descriptive statistics (and BariClot analysis)

open **`part4_descriptive-statistics-bariclot`** and run all cells:

* create summary statistics of the entire dataset using
* compare training, validation, and testing sets for both leak and clot
* calculate bariclot score for each patient in the clot training set
* this will generate:
  * a **`results/`** directory
  * a **`results/descriptive_stats subdirectory`** - holds all descriptive data generated by tableone:  **`dataset_summary.csv`** (manuscript table 1), **`leak_set_summary.csv`** (supplementary table 1), and **`clot_set_summary.csv`** (supplementary table 2)
* a **`results/bariclot`** subdirectory - holds bariclot results for the test cohort (**`bariclot_test.csv`**). we also run a sanity check for bariclot on 2015 data (**`bariclot_2015.csv`**) to make sure it performs as expected in this directory.
    
## 8. train, validate, and test models

open **`part5_models.ipynb`** and run all cells. 

**Warning**: this is a long run (about 6-10 hours on a late-2016 Macbook Pro). this trains and evaluates artificial neural network, gradient boosting machine, and logistic regression models for each outcome

* the `run_models` function builds all three models for each specified outcome. this function contains some rudimentary progress monitoring. it allows the analyst to see what models have been trained and evaluated. 
* the function also prints a running tally of number of training epochs along with the best validation (but not testing) AUC as the neural net trains. it outputs inline graphs to show training loss, validation loss, and validation AUCs over time once training is complete.
* scaled and normalized training data (both oversampled and original), validation data, and testing data is saved in the **`results/study_data/`** directory. this is so that it can be used in the following notebook in order to build logistic regression models.logistic regression models built in this notebook are discarded and not reported because the sklearn package does not give confidence intervals around coefficients, and we wished to report this in the supplement.
* as part of neural net training, we evaluate the model against the validation data after each training epoch, and if the model outperforms all previous models, we save it, iteratively overwriting the previous best-performing model. the best leak model is stored as **`results/best_ann_LEAK/best_ann_LEAK.pt`** and the best clot model is stored as **`results/best_ann_CLOT/best_ann_CLOT.pt`**. we deliberately permit our models to overfit the training data until their validation performance definitively deteriorates (using  automatic early stopping criteria), and then we recover the best model to use in testing.
* models outputs for predicting for leak and clot are saved in **`results/study_models_LEAK/`** and **`results/study_models_CLOT/`** respectively. In each of these folders, model performance against training, validation, and testing data is saved in the format: **`OUTCOME_dataset_FINAL.csv`**, e.g., **`LEAK_test_FINAL.csv`**. 
* Variable importance in the gradient boosting machine models for leak and clot are saved in the corresponding **`study_models_`** directories as **`LEAK_xgb_var_imp.csv`** and **`LEAK_xgb_var_imp.csv`** respectively 

## 8. build logistic regression models

open **part6_logistic_regression.ipynb`** and run all cells

*  we added this notebook for the purposes of responding to peer review and wanted to change the prior notebook as little as possible so that we did not change our results. therefore, data that is split and standardized in the previous notebook is saved, and then opened here to run new logistic regression models. the reason we run new logistic regression models is to get confidence intervals for coefficients to calculate odds ratios using the statsmodels package, which we did not use to generate our initial submission.

## 9. analyze outputs, generate figures 

open **`part7_data_processing.R`** in RStudio and run the entire script. 

**Warning**: this is a long run (about 6-10 hours on a late-2016 Macbook Pro). We've posted the code with an option to execute the overnight run set to false. By far the longest part of the run  generates model performances with specificities held as close to 97.5% as possible. To generate full results, set `overnight_run = TRUE` (cell #2). 

Important outputs of this script:

* **`auc_cis_table`**: an R object containing AUCs and confidence intervals for each model on the testing data
* **`compare_table`**: an R object containing p values for each pairwise comparison of model performance (as measured by AUC on the testing data) for both outcomes
* **figures 1-4** from the manuscript are saved in the **`results/figures/`** folder, which is created by this script. figures are saved in both low-resolution and high-resolution formats
* **partial AUCs on the left side of the ROC curve** with specificity > 90% are generated and outputted on lines 257-256 of this script
* **`performance_by_dataset_table`**: an R object containing model performance in training, testing, and validation sets for both outcomes of interest (supplementary table 3)
* **`thresholds_table`**: R object containing performance characteristics of the artificial neural network, gradient boosting machine, and logistic regression models for predicting each outcome (manuscript tables 2 & 3)

## notes:

1. _The Metabolic and Bariatric Surgery Accreditation and Quality Improvement Program (MBSAQIP) the hospitals participating in the MBSAQIP are the source of the data used herein; they have not verified and are not responsible for the statistical validity of the data analysis or the conclusions derived by the authors._

2. BariClot was originally reported in Dang JT, Switzer N, Delisle M, Laffin M, Gill R, Birch DW, Karmali S. Predicting venous thromboembolism following laparoscopic bariatric surgery: development of the BariClot tool using the MBSAQIP database. Surg Endosc. 2018; PMID:30003351; http://dx.doi.org/10.1007/s00464-018-6348-0
