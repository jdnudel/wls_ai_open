{"cells":[{"source":["from IPython import get_ipython\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" # part 1: parse original data\n","\n"," this notebook contains code to parse raw mbsaqip files after they have been uncompressed locally\n","\n"," note that this is designed to merge tables in a way that allows us to specify composite outcomes of interest to the present study."],"metadata":{}},{"source":["get_ipython().run_line_magic('matplotlib', 'inline')\n","get_ipython().run_line_magic('reload_ext', 'autoreload')\n","get_ipython().run_line_magic('autoreload', '2')\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["import numpy as np\n","import pandas as pd\n","import os\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["# Set ipython's max row display\n","pd.set_option('display.max_row', 100)\n","\n","# Set iPython's max column display\n","pd.set_option('display.max_columns', 50)\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["PATH = 'mbsaqip_originals/'\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" * uncomment this if building and joining from original data tables"],"metadata":{}},{"source":["PATH_YEARS = ['2015/', '2016/', '2017/']\n","table_names = ['main', 'intv', 'bmi', 'reop', 'read']\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" ### step 1: load from txt files and merge tables within each year\n","\n"," in the following loop, for each year of data:\n","\n"," * load the years data from csv files\n"," * merge tables to get data needed for  composite endpoint as defined in Dang et al Bariclot paper\n","  * readmission for dvt or pe\n","  * reoperation for dvt or pe\n","  * reintervention for dvt or pe\n"," * save  resultant dataframe to new .csv\n","\n"," uncomment this to build merged data tables for each year"],"metadata":{}},{"source":["for year in PATH_YEARS:\n","    for fname in table_names:\n","        df_main = pd.read_csv(f'{PATH}{year}main.txt', sep='\\t', low_memory=False)\n","        df_read = pd.read_csv(f'{PATH}{year}read.txt', sep='\\t', low_memory=False)\n","        df_reop = pd.read_csv(f'{PATH}{year}reop.txt', sep='\\t', low_memory=False)\n","        df_intv = pd.read_csv(f'{PATH}{year}intv.txt', sep='\\t', low_memory=False)\n","        df_bmi  = pd.read_csv(f'{PATH}{year}bmi.txt' , sep='\\t', low_memory=False)\n","        \n","        df_main = pd.merge(df_main,\n","                            df_read[['CASEID', 'SUSPREASON']],\n","                            on='CASEID',\n","                            how='left')\n","        \n","        df_main = pd.merge(df_main,\n","                            df_reop[['CASEID', 'REOP_SUSPECTED_REASON_BAR']],\n","                            on='CASEID',\n","                            how='left')\n","        \n","        df_main = pd.merge(df_main,\n","                            df_intv[['CASEID', 'INTV_REASON_BAR']],\n","                            on='CASEID',\n","                            how='left')\n","        \n","        df_main.to_csv(f'{PATH}{year}year_joined.csv')\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" ### step 3: load merged csv file for each year and concatenate\n","\n"," * load each year of data\n"," * inspect and compare the columns for each year\n"," * fix one header that is inconsistent between years\n","  * `agegt80` in 2015 is called  `ageGT80` in 2016 and 2017\n","  * does not affect this analysis but prove useful to consolidate this column for other analyses\n"," * drop non-intersecting columns"],"metadata":{}},{"source":["df_joined_fiv = pd.read_csv(f'{PATH}2015/year_joined.csv', low_memory=False, index_col=0)\n","df_joined_six = pd.read_csv(f'{PATH}2016/year_joined.csv', low_memory=False, index_col=0)\n","df_joined_sev = pd.read_csv(f'{PATH}2017/year_joined.csv', low_memory=False, index_col=0)\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["print(len(df_joined_fiv.columns))\n","print(len(df_joined_six.columns))\n","print(len(df_joined_sev.columns))\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["df_joined_fiv['ageGT80'] = df_joined_fiv['agegt80'] \n","df_joined_fiv = df_joined_fiv.drop(columns=['agegt80'])\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["six_not_fiv = df_joined_fiv.columns ^ df_joined_six.columns\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["df_joined_six = df_joined_six.drop(columns=six_not_fiv)\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["sev_not_six = df_joined_six.columns ^ df_joined_sev.columns\n","df_joined_sev = df_joined_sev.drop(columns=sev_not_six)\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["df_all_years = pd.concat([df_joined_fiv, df_joined_six, df_joined_sev], sort=False)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" ### step 4: quick data integrity test\n","\n"," make sure we got the right number of total patients:"],"metadata":{}},{"source":["print(len(df_joined_fiv), len(df_joined_six), len(df_joined_sev))\n","print(len(df_joined_fiv) + len(df_joined_six) + len(df_joined_sev))\n","print(len(df_all_years))\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" ### step 5: write concatenated data to a new file"],"metadata":{}},{"cell_type":"markdown","source":[" this will throw an error if it runs after the '~/all_years' directory has been built; if that happens just delete the directory (or write some additional code for better file handling)."],"metadata":{}},{"source":["# make a dir to hold data from all years\n","os.mkdir(f'{PATH}all_years')\n","\n","# save the data\n","df_all_years.to_csv(f'{PATH}all_years/all_years.csv')\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" the data for all available years is now concatenated"],"metadata":{}}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}