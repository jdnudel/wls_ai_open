{"cells":[{"source":["from IPython import get_ipython\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" # part 2: population and feature selection"],"metadata":{}},{"cell_type":"markdown","source":[" * load original, minimally processed, consolidated study data\n"," * select target population via screening and exclusion\n"," * select features including preliminary feature engineering\n"," * create a new csv to hold study data"],"metadata":{}},{"source":["get_ipython().run_line_magic('matplotlib', 'inline')\n","get_ipython().run_line_magic('reload_ext', 'autoreload')\n","get_ipython().run_line_magic('autoreload', '2')\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["#np.sum([83246 ,33888 ,7235 ,708,9 ,74 ,266 ,731] )\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["import pandas as pd\n","import numpy as np\n","import os\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["np.set_printoptions(threshold=50, edgeitems=20)\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["# Set ipython's max row display\n","pd.set_option('display.max_row', 100)\n","\n","# Set iPython's max column display\n","pd.set_option('display.max_columns', 50)\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["PATH = 'study_data/'\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["df_main = pd.read_csv('mbsaqip_originals/all_years/all_years.csv', low_memory=False, index_col=0)\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["total_number_of_candidates_for_inclusion = len(df_main); total_number_of_candidates_for_inclusion\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" set seed to make results reproducible\n","\n"," * this is used for sampling uniform distributions for 2015 age variables, explained below and in our manuscript"],"metadata":{}},{"source":["np.random.seed(0)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" dict to hold reasons for exclusion for use in flow chart"],"metadata":{}},{"source":["exclusions = {}\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" specify the outcomes and feature set to be used in predictive models"],"metadata":{}},{"source":["USE_VARS = ['LEAK', \n","            'CLOT', \n","            'SEX', \n","            'race_PUF', \n","            'hispanic', \n","            'CPT', \n","            'GERD', \n","            'MOBILITY_DEVICE', \n","            'HISTORY_DVT', \n","            'MI_ALL_HISTORY', \n","            'PTC', \n","            'PCARD', \n","            'HIP', \n","            'HTN_MEDS', \n","            'HYPERLIPIDEMIA', \n","            'VENOUS_STASIS', \n","            'DIALYSIS', \n","            'RENAL_INSUFFICIENCY', \n","            'THERAPEUTIC_ANTICOAGULATION', \n","            'DIABETES', \n","            'SMOKER', \n","            'FUNSTATPRESURG', \n","            'COPD', \n","            'OXYGEN_DEPENDENT', \n","            'HISTORY_PE', \n","            'SLEEP_APNEA', \n","            'CHRONIC_STEROIDS', \n","            'IVCF', \n","            'ASACLASS', \n","            'ASSISTANT_TRAINING_LEVEL', \n","            'OPYEAR', \n","            'HYPOALB', \n","            'ANEMIA', \n","            'HGTCM', \n","            'BMI_CONSOL', \n","            'BMI_DELTA', \n","            'WGTKG', \n","            'OPLENGTH', \n","            'AGE_CONSOL', \n","            'ALBUMIN', \n","            'HCT'\n","]\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" the cpt codes we care about are:\n","\n"," * lap gastric bypass\n","  * 43644\n","  * 43645\n","\n"," * lap sleeve gastrectomy\n","  * 43775"],"metadata":{}},{"source":["include_cpts = [43644, 43645, 43775] \n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["df_exclude_cpts = df_main[~df_main['CPT'].isin(include_cpts)]\n","exclusions['wrong cpt'] = len(df_exclude_cpts)\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["df_main = df_main[df_main['CPT'].isin(include_cpts)]\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" Alizadeh: \"The study population also excluded patients who were under 18Â years of age, undergoing revisional bariatric surgery, had previous bariatric or foregut surgery, and undergoing emergency surgery\""],"metadata":{}},{"cell_type":"markdown","source":[" exclude revisional cases"],"metadata":{}},{"source":["exclusions['revisional cases'] = df_main['CPTUNLISTED_REVCONV'].value_counts(dropna=False)[1]\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["df_main = df_main[df_main['CPTUNLISTED_REVCONV'] == 0]\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" exclude patients with history of previous bariatric or foregut surgery"],"metadata":{}},{"source":["exclusions['previous obesity or foregut surgery'] = df_main['PREVIOUS_SURGERY'].value_counts(dropna=False)['Yes']\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["df_main = df_main[df_main['PREVIOUS_SURGERY'] == 'No']\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" exclude children"],"metadata":{}},{"source":["df_kids = df_main[df_main['AGE'] < 18.0]\n","df_elderly = df_main[df_main['AGE'] >= 80.0]\n","df_confused = df_main[~((df_main['AGE'] >= 18.0) | (df_main['AGE'] < 18.0) | (df_main['AGE'] >= 80.0))]\n","df_main = df_main[(df_main['AGE'] >= 18.0) & (df_main['AGE'] < 80.0)]\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["exclusions['age under 18'] = len(df_kids)\n","exclusions['age 80 or over'] = len(df_elderly)\n","exclusions['age not available'] = len(df_confused)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" exclude emergency cases"],"metadata":{}},{"source":["exclusions['emergency surgery'] = len(df_main[df_main['PRIORITY'] == 'Yes'])\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["df_main = df_main[df_main['PRIORITY'] == 'No']\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["print(len(df_main))\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" consolidate lap gastric bypass procedures"],"metadata":{}},{"source":["df_main['CPT'] = np.where(df_main['CPT'] == 43645, 43644, df_main['CPT'])\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" there is no reason to consolidate age variables because the max age in the data is 80,  there is no patient without an age, and none of the remaining patients have an age greater than 80.\n","\n"," for these reasons we can safely ignore the age > 80 variable as well"],"metadata":{}},{"source":["df_main.groupby('OPYEAR')['ageGT80'].value_counts(dropna=False)\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["print('the max age in the data is', max(df_main['AGE']))\n","print('there are', df_main['AGE'].isna().sum(), 'patients without an age')\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" 2016 and 2017 have precise age and 2015 has just age floor. options include: take the floor of all ages; do a 'half cycle correction' and take the midpoint of all ages. my preferred option is to draw uniform samples between age and age + 1 among patients in 2015. this minimizes the overall error in age without introducing unnatural patterns into the data. this allows us to cluster ages into categories later without any issues"],"metadata":{}},{"source":["df_main['AGE_CONSOL'] = np.where(df_main['OPYEAR'] == 2015, \n","                                 df_main['AGE'] + np.random.uniform(0,1,1),\n","                                 df_main['AGE'])\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" consolidate IVC filter variables"],"metadata":{}},{"source":["df_main['IVC_FILTER'].value_counts(dropna=False)#.sum()\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["df_main['IVC_TIMING'].value_counts(dropna=False)\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["df_main['IVCF'] = np.where(df_main['IVC_FILTER'] == 'Yes', df_main['IVC_TIMING'], df_main['IVC_FILTER'])\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" dealing with missing height and weight data\n","\n"," first, look to see if we can calculate BMI from weight in the event that BMI is not available (we can't)"],"metadata":{}},{"source":["# exclude cases with no information on BMI or weight\n","df_bmi_or_wgt_info = df_main[~(df_main['BMI_HIGH_BAR'].isna() & \n","                df_main['WGT_HIGH_BAR'].isna() & \n","                df_main['WGT_CLOSEST'].isna()  & \n","                df_main['BMI'].isna())]\n","\n","# exclude cases with no information on BMI  \n","df_bmi_info = df_main[~(df_main['BMI_HIGH_BAR'].isna() & \n","                df_main['BMI'].isna())]\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["if len(df_bmi_or_wgt_info) == len(df_bmi_info):\n","    print('there are no missing BMI columns for which we can calculate BMI from weight')\n","    print('get rid of cases where there is no information on BMI or weight')\n","    print('we lose', len(df_main) - len(df_bmi_or_wgt_info), 'cases')\n","    exclusions['no info on bmi or weight'] = len(df_main) - len(df_bmi_or_wgt_info)\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["df_main = df_bmi_or_wgt_info.copy()\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" next, look at cases with incomplete BMI information. For example, where most recent BMI is available but max BMI is not, or vice versa"],"metadata":{}},{"source":["df1 = df_main[df_main['BMI_HIGH_BAR'].isna() & (~df_main['BMI'].isna())]\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["df2 = df_main[df_main['BMI'].isna() & (~df_main['BMI_HIGH_BAR'].isna())]\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["print('there are', len(df1), 'cases where most recent BMI is available but max BMI is not')\n","print('there are', len(df2), 'cases where max BMI is available but most recent BMI is not')\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" where most recent BMI is not available, assume max BMI"],"metadata":{}},{"source":["df_main['BMI_CONSOL'] = np.where(df_main['BMI'].isna(), df_main['BMI_HIGH_BAR'], df_main['BMI'])\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["df_main[['HGT', 'BMI_CONSOL']].isna().sum()\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" BMI change (may be a useful feature)"],"metadata":{}},{"source":["df_main['BMI_DELTA'] = df_main['BMI_CONSOL'] - df_main['BMI_HIGH_BAR']\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" in cases where we can't calculate the delta, we will assume no change. for now, leave as NaN so that we can build Table 1 in a subsequent notebook. NaNs will get set to the population average in a later processing step."],"metadata":{}},{"source":["print('in', df_main['BMI_DELTA'].isna().sum(), 'cases, we cannot calculate the delta.')\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" convert all heights to cm"],"metadata":{}},{"source":["df_main['HGTCM'] = np.where(df_main['HGTUNIT']=='cm', df_main['HGT'], df_main['HGT'] * 2.54)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" back-calculate weight from height and bmi"],"metadata":{}},{"source":["df_main['WGTKG'] = ((df_main['HGTCM']/100) ** 2) * df_main['BMI_CONSOL']\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" looking at distribution of cases by year now that all exclusions have been applied"],"metadata":{}},{"source":["df_main.groupby('OPYEAR')['CPT'].value_counts(dropna=False)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" dealing with missing variables for albumin, hct, and asa class."],"metadata":{}},{"source":["print('number of cases missing albumin:',df_main['ALBUMIN'].isna().sum())\n","print('number of cases missing hematocrit:',df_main['HCT'].isna().sum())\n","print('number of cases missing asa class:',df_main['ASACLASS'].isna().sum())\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" albumin lowest quintile and distribution by year"],"metadata":{}},{"source":["df_main['ALBUMIN'].quantile(q=0.2,  interpolation='linear')\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["df_main['ALBUMIN'].hist(by=df_main['OPYEAR'], bins = 7)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" hct lowest quintile and distribution by year"],"metadata":{}},{"source":["df_main['HCT'].quantile(q=0.2,  interpolation='linear')\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["df_main['HCT'].hist(by=df_main['OPYEAR'], bins = 20)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" because there are so many missing hct and albumin values, it may make sense to experiment with treating them as categorical, in which case we could specify an 'unknown' category.\n","\n"," similarly, since ASA class is categorical, we can specify an unknown category (or experiment with just getting rid of those cases in some models)\n","\n"," create categories for anemia and hypoalbuminemia. for the former, we take the lowest quintile. for the latter, we take all albumin under 3.5 based on the methods from alizadeh"],"metadata":{}},{"source":["df_main['ANEMIA_temp'] = np.where(df_main['HCT'] < 38, 1, 2)\n","df_main['ANEMIA']      = np.where(df_main['HCT'].isna(), 0, df_main['ANEMIA_temp'])\n","\n","df_main['HYPOALB_temp'] = np.where(df_main['ALBUMIN'] < 3.5, 1, 2)\n","df_main['HYPOALB']      = np.where(df_main['ALBUMIN'].isna(), 0, df_main['ANEMIA_temp'])\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" make sure we have the correct number of missing anemia and hypoalbuminema values by ensuring they match the number of missing hct and alb values above"],"metadata":{}},{"source":["print(len(df_main[df_main['ANEMIA']==0]))\n","print(len(df_main[df_main['HYPOALB']==0]))\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" convert null asa class values to separate 'unknown' category for easier data handling"],"metadata":{}},{"source":["df_main['ASACLASS'] = np.where(df_main['ASACLASS'].isna(), 'Unknown', df_main['ASACLASS'])\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" composite endpoints for clot and leak as defined respectively in Dang et al and Alizadeh et al"],"metadata":{}},{"source":["rationale_clot = ['Vein Thrombosis Requiring Therapy', 'Pulmonary Embolism']\n","rationale_leak = ['Anastomotic/Staple Line Leak']\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["yes = ['Yes']\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["df_composite_clot = (df_main['SUSPREASON'].isin(rationale_clot) |\n","                df_main['REOP_SUSPECTED_REASON_BAR'].isin(rationale_clot) |  \n","                df_main['INTV_REASON_BAR'].isin(rationale_clot) |\n","                df_main['VEINTHROMBREQTER'].isin(yes) |\n","                df_main['DEATH_CAUSE_BAR'].isin(rationale_clot))\n","\n","df_composite_leak = (df_main['SUSPREASON'].isin(rationale_leak) |\n","                df_main['REOP_SUSPECTED_REASON_BAR'].isin(rationale_leak) |  \n","                df_main['INTV_REASON_BAR'].isin(rationale_leak) |\n","                df_main['OSSIPATOS'].isin(yes) |\n","                df_main['DRAIN_PRESENT_30DAY_BAR'].isin(yes))\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["df_main['CLOT'] = df_composite_clot\n","df_main['LEAK'] = df_composite_leak\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["df_main['CLOT'].value_counts(dropna=False)[True]\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["df_main['LEAK'].value_counts(dropna=False)[True]\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" select down to only the columns we may wish to include in building models"],"metadata":{}},{"source":["df_main = df_main[USE_VARS].copy()\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" check for columns with missing values"],"metadata":{}},{"source":["df_main[df_main.columns].isna().sum()\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" re-index"],"metadata":{}},{"source":["df_main = df_main.reset_index(drop=True) # `drop=True` drops the pre-existing, out of order index, which is included by default\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["exclusions\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" confirm we tallied up all exclusions correctly:"],"metadata":{}},{"source":["if total_number_of_candidates_for_inclusion - sum(exclusions.values()) - len(df_main) == 0:\n","    print('this confirms that the total size of the database minus the size of the exclusions is equal to the size of the analysis cohort')\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" save data\n","\n"," as in part 1, this will throw an error if it runs after the '~/all_years' directory has been built; if that happens just delete the directory (or write some additional code for better file handling)."],"metadata":{}},{"source":["# make a dir to hold data from all years\n","os.mkdir(f'{PATH}')\n","\n","#\n","df_main.to_csv(f'{PATH}/study_data.csv')\n","\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}